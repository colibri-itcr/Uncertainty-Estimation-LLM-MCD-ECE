# Uncertainty Estimation in Large Language Models to Support Biodiversity Conservation
María Mora-Cross and Saúl Calderón-Ramírez, 
Costa Rica Institute of Technology, {maria.mora, sacalderon}@itcr.ac.cr


This study introduces an exploratory analysis of the application of Monte Carlo Dropout (MCD),Perplexity, and Expected Calibration Error (ECE) techniques to evaluate the uncertainty of generative large language models (LLMs). The investigation focuses on two widely accessible language models, namely Falcon-7B and DistilGPT-2.

This files contains the functions used to run the experiments that support the research results of the publication: Maria Mora-Cross and Saul Calderon-Ramirez (2024). Uncertainty Estimation in Large Language Models to Support Biodiversity Conservation. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.

Two language models were used GPT-2 and Falcon-7B. The results of all experiments were stored in a PostgreSQL database. Please note that after the project's completion, the authors discovered that the ELI5 dataset is no longer available on Hugging Face. The dataset page has the message: "Dataset eli5 is defunct and no longer accessible due to unavailability of the source data".

